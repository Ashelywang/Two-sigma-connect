{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images_sample\n",
      "sample_submission.csv\n",
      "test.json\n",
      "train.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import json\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"./input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "data_train = pd.read_json(open(\"./input/train.json\"),'r')\n",
    "data_test = pd.read_json(open(\"./input/test.json\"),'r')\n",
    "submitexam = pd.read_csv('./input/sample_submission.csv')\n",
    "\n",
    "from bs4 import BeautifulSoup  \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_most_common(dataset,c_name,n):\n",
    "    cnt = Counter()\n",
    "    datacolumn = dataset[c_name]\n",
    "    #First Create a counter and find the top n most frequent words\n",
    "    for eachr in datacolumn:\n",
    "        if type(eachr)!= list:\n",
    "            for single_word in eachr.split():\n",
    "                cnt[single_word.lower()] +=1\n",
    "        else:\n",
    "            for word in eachr:\n",
    "                for single_word in word.split():\n",
    "                    cnt[single_word.lower()] +=1\n",
    "    \n",
    "    Most_Common_W = cnt.most_common(n)\n",
    "    return Most_Common_W\n",
    "\n",
    "\n",
    "\n",
    "def get_feature_matrix(Most_Common_W,dataset,c_name):\n",
    "    n = len(Most_Common_W)\n",
    "    datacolumn = dataset[c_name]\n",
    "    #Then Create a data frame collecting 0-1 dummy variables\n",
    "    dummy_data = np.zeros(shape=(len(datacolumn),n))\n",
    "    d = pd.DataFrame(dummy_data, columns=dict(Most_Common_W).keys())\n",
    "    \n",
    "    for ieachr in range(len(datacolumn)):\n",
    "        if ieachr %10000==0:\n",
    "            print ieachr,'/',len(datacolumn)\n",
    "        eachr = datacolumn.iloc[ieachr]\n",
    "        if type(eachr) == list:\n",
    "            for feature_word in ' '.join(datacolumn.iloc[ieachr]).lower().split():\n",
    "                if feature_word in dict(Most_Common_W).keys():\n",
    "                    d[feature_word].iloc[ieachr] = 1\n",
    "        else:\n",
    "            for feature_word in eachr.lower().split():\n",
    "                if feature_word in dict(Most_Common_W).keys():\n",
    "                    d[feature_word].iloc[ieachr] = 1\n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "def get_numerical_feature(dataset, col_name):\n",
    "    Newfeature = dataset[col_name]\n",
    "    return Newfeature\n",
    "\n",
    "def prob_to_csv(proba,colname,listing_id):\n",
    "    pb = pd.DataFrame(proba)\n",
    "    pb.columns = colname\n",
    "    pb.to_csv(\"file_path.csv\")\n",
    "\n",
    "\n",
    "\n",
    "def find_freq_in_different_levels(data,level):\n",
    "    data_low,nlow = find_freq(data[level =='low'])\n",
    "    data_mid,nmid = find_freq(data[level =='medium'])\n",
    "    data_high,nhigh = find_freq(data[level =='high'])\n",
    "    return data_low,data_mid,data_high\n",
    "    \n",
    "        \n",
    "    \n",
    "def find_freq(data):\n",
    "    cnt = Counter()\n",
    "    ndata = len(data)\n",
    "    for eachr in data:\n",
    "        for single_word in eachr.split():\n",
    "            cnt[single_word.lower()] +=1\n",
    "    cnt = dict(cnt)\n",
    "    return cnt,ndata\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 49352\n",
      "10000 / 49352\n",
      "20000 / 49352\n",
      "30000 / 49352\n",
      "40000 / 49352\n",
      "0 / 49352\n",
      "10000 / 49352\n",
      "20000 / 49352\n",
      "30000 / 49352\n",
      "40000 / 49352\n"
     ]
    }
   ],
   "source": [
    "\n",
    "subcolumn_name = ['bathrooms','bedrooms','latitude','longitude','price']\n",
    "N_feature = get_numerical_feature(data_train,subcolumn_name)\n",
    "M_features = get_most_common(data_train,'features',500)\n",
    "M_descrip = get_most_common(data_train,'description',500)\n",
    "Dummy_feature = get_feature_matrix(M_features,data_train,'features')\n",
    "Dummy_feature2 = get_feature_matrix(M_descrip,data_train,'description')\n",
    "Dummy_feature.to_pickle('Dummy1.pkl')\n",
    "Dummy_feature2.to_pickle('Dummy2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 74659\n",
      "10000 / 74659\n",
      "20000 / 74659\n",
      "30000 / 74659\n",
      "40000 / 74659\n",
      "50000 / 74659\n",
      "60000 / 74659\n",
      "70000 / 74659\n",
      "0 / 74659\n",
      "10000 / 74659\n",
      "20000 / 74659\n",
      "30000 / 74659\n",
      "40000 / 74659\n",
      "50000 / 74659\n",
      "60000 / 74659\n",
      "70000 / 74659\n"
     ]
    }
   ],
   "source": [
    "Dummy_feature_test = get_feature_matrix(M_features,data_test,'features')\n",
    "Dummy_feature2_test = get_feature_matrix(M_descrip,data_test,'description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Dummy_feature_test.to_pickle('Dummy_feature_test.pkl')\n",
    "Dummy_feature2_test.to_pickle('Dummy_feature2_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class naive_bayes_trainer:\n",
    "    \n",
    "    class Data2:\n",
    "        def __init__(self,feature,label):\n",
    "            self.data = feature\n",
    "            self.label = label\n",
    "        def split_train_test(self):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(self.data.index, self.label,test_size=0.3, random_state=20)\n",
    "            self.xtrain = self.data.iloc[X_train]\n",
    "            self.xtest = self.data.iloc[X_test]\n",
    "            self.y_train = y_train\n",
    "            self.y_test = y_test\n",
    "            \n",
    "    def __init__(self,feature,label):\n",
    "        self.data = self.Data2(feature,label)\n",
    "        self.data.split_train_test()\n",
    "    \n",
    "    def naive_bayes_prob(self):\n",
    "        BN_NB = MultinomialNB()\n",
    "        self.model = BN_NB.fit(self.data.xtrain,self.data.y_train)\n",
    "        return self.model\n",
    "    \n",
    "    def accuracy(self):\n",
    "        y_pred = self.model.predict(self.data.xtest)\n",
    "        matrix = confusion_matrix(self.data.y_test, y_pred)\n",
    "        return matrix\n",
    "    def predict(self, testdata):\n",
    "        y_pred = self.model.predict(testdata)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def t_to_csv(prob,cname,indexi):\n",
    "    prob_matrix = pd.DataFrame(prob,index=indexi,columns=cname)\n",
    "    prob_matrix.to_csv('./submit.csv',sep=\",\",index = True,index_label='listing_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dummy = pd.concat([Dummy_feature2, Dummy_feature], axis=1)\n",
    "NB = naive_bayes_trainer(Dummy,data_train['interest_level'])\n",
    "NB.naive_bayes_prob()\n",
    "NB.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.32798005e-01,   3.24917605e-01,   4.42284390e-01],\n",
       "       [  9.61009357e-02,   7.52421809e-01,   1.51477255e-01],\n",
       "       [  3.28368269e-01,   3.40626344e-01,   3.31005387e-01],\n",
       "       ..., \n",
       "       [  5.21538957e-03,   8.21059407e-01,   1.73725203e-01],\n",
       "       [  9.31909715e-01,   8.24821151e-03,   5.98420737e-02],\n",
       "       [  7.17202517e-06,   9.99992201e-01,   6.27131569e-07]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dummy_test = pd.concat([Dummy_feature2_test, Dummy_feature_test], axis=1)\n",
    "prob = NB.model.predict_proba(Dummy_test)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_to_csv(prob,['high','low','medium'],data_test.listing_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
